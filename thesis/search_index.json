[
["results.html", "Chapter 4 Results 4.1 Benchmark datasets 4.2 Faktion datasets", " Chapter 4 Results 4.1 Benchmark datasets 4.1.1 CoNLL2003 - English The best results obtained on the CoNLL2003 task (Fig 4.1) using state-of-the-art contextualized embeddings from pretrained LMs were similar to the state-of-the-art results reported in literature (Table 4.1). As a proof of concept to demonstrate the added value of “transfer learning” in the context of NER, the BiLSTM-CRF classifier was trained without any input from pretrained (either static or contextualized) embeddings, i.e. only using randomly initialized embeddings that were updated during training to minimize the loss w.r.t. to the NER objective. Both the one hot word type and character-feature embeddings performed poorly, with F1-scores of 74.0 % and 75.4 %, respectively (Fig 4.1). Performance improved drastically when using static BytePair or fastText embeddings alone (Fig 4.1). However, as expected, best performance was provided when contextualized representations from pretrained LMs were used, with the performance obtained with embeddings based on BERT, Flair or both being similar when they were concatenated into large vectors containing all static and task-specific embedding types evaluated here (Table 4.1). While the monolingual embeddings appeared to outperform the multilingual embeddings, the differences between large stackings of respectively BERT and mBERT embeddings were surprisingly small (F1-scores of 92.1 % vs. 91.4 %, respectively), with mBERT-based embeddings being clearly superior to mFlair-based embeddings (Table 4.1). Interestingly, adding mFlair embeddings to the stacked embedding consisting of mBytePair, OHE and character embeddings (F1-score: 88.0 %) did not improve performance (F1-score: 87.9 %) (Fig 4.1, Table 4.1). Figure 4.1: Test-set micro-average precision, recall and F1-score for monolingual (top) and multilingual (bottom) embeddings on the English CoNLL2003 NER task. The results are grouped according to the constituents of each (stacked) embedding, consistent with Table ??. The contextualized embedding type, if present, is indicated on the different panels from left to right. The stacked/task-specific embedding type (if present) is shown on the x-axis. For clarity, the results for the language-independent OHE and Char embeddings are indicated on both the upper (monolingual) and lower (multilingual) panels. Char: character embeddings; OHE: One Hot Embeddings; BPEmb: BytePair embeddings; fastT: fastText embeddings; All: all embeddings indicated on the x-axis for this panel concatenated into a single vector. Table 4.1: Results acquired on the English CoNLL2003 task with monolingual and multilingual NER systems. The results for the best-performing model for the original CoNLL2003 shared task is included for comparison (Florian et al. 2003). A complete overview of all results is provided in Table ??. Description Precision Recall F1-score Monolingual CoNLL2003 best - IBM Florian 89.0 88.5 88.7 Flair - BiLSTM-CRF 93.1 BERT base - finetuned 92.4 90.8 90.9 90.9 BERT 91.3 91.1 91.2 BERT + all-en 91.9 92.3 92.1 Flair 90.3 90.8 90.5 Flair + all-en 92.3 92.4 92.3 BERT + Flair 91.3 91.9 91.6 BERT + Flair + all-en 92.3 92.7 Multilingual mBERT - finetuned 92.0 88.3 87.8 88.0 mBERT 90.8 90.6 90.7 mBERT + all 91.0 91.9 mFlair 84.6 84.8 84.7 mFlair + all 87.6 88.2 87.9 mBERT + mFlair 90.5 91.2 90.9 mBERT + mFlair + all 90.8 91.4 91.1 References a Florian et al (2003) b Akbik et al (2018) c Devlin et al (2019) d Wu et al (2019) 4.1.2 CoNLL2002 - Dutch Results were similar for the Dutch CoNLL2002 task (Fig 4.2, Table 4.2) in the sense that task-specific representations (i.e. character embeddings (F1: 67.8 %) or OHE (F1: 61.7 %)) were clearly inferior to embeddings obtained from pretrained LMs, with the best-performing monolingual system being based on the large (i.e. 5614-dimensional) concatenations of embeddings from both BERTje and Dutch Flair embeddings, stacked with Dutch fastText, Dutch BytePair and task-specific OHE and character embeddings (F1: 93.0 %, Table 4.2). Even though this is the result of only a single training run, all the monolingual embeddings based on BERTje consistently outperformed the result reported in the BERTje paper itself (F1: 90.2 %) (de Vries et al. 2019) as well as the current best result on the CoNLL2002 task, obtained by the fine-tuned mBERT model (F1-score: 90.9 %) (Wu and Dredze 2019). In contrast to the English CoNLL2003 task, monolingual embeddings from the Flair model performed poorly as compared to the BERT-based embeddings, with the difference being even more pronounced when comparing mFlair-based embeddings with mBERT-based embeddings (Table 4.2). Figure 4.2: Test-set micro-average precision, recall and F1-score for monolingual (top) and multilingual (bottom) embeddings on the Dutch CoNLL2002 NER task. The results are grouped according to the constituents of each (stacked) embedding, consistent with Table ??. The contextualized embedding type, if present, is indicated on the different panels from left to right. The stacked/task-specific embedding type (if present) is shown on the x-axis. For clarity, the results for the language-independent OHE and Char embeddings are indicated on both the upper (monolingual) and lower (multilingual) panels. Char: character embeddings; OHE: One Hot Embeddings; BPEmb: BytePair embeddings; fastT: fastText embeddings; All: all embeddings indicated on the x-axis for this panel concatenated into a single vector. Table 4.2: Results acquired on the Dutch CoNLL2002 task with monolingual and multilingual NER systems. The best-performing model for the original CoNLL2002 shared task (Carreras, M‘arquez, and Padr’o 2002) is included for comparison. A complete overview of all results is provided in Table ??. Description Precision Recall F1-score Monolingual CoNLL2002 best - AdaBoost 77.8 76.3 77.1 Flair NL - BiLSTM-CRF 89.6 fine-tuned BERTje 90.2 88.1 86.8 87.4 BERTje 91.8 91.3 91.5 BERTje + all-nl 93.1 92.2 92.6 Flair 86.9 86.2 86.4 Flair + all-nl 90.2 89.3 89.7 BERTje + Flair 92.4 92.5 92.4 BERTje + Flair + all-nl 93.1 93.0 Multilingual fine-tuned mBERT 90.9 82.8 80.0 81.3 mBERT 89.2 88.5 88.8 mBERT + all 90.6 89.4 90.0 mFlair 79.8 78.8 79.1 mFlair + all 84.7 82.6 83.6 mBERT + mFlair 89.8 89.4 89.5 mBERT + mFlair + all 90.4 89.8 References a Carreras et al 2002 b GitHub: stefan-it/flair-experiments c GitHub: wietsedv/bertje d Wu et al (2019) 4.1.3 WikiNER - French To evaluate how monolingual and multilingual embeddings performed on a French NER task, we used a small subset of the French WikiNER dataset. Even though the entity classes (i.e. PER, ORG, LOC and MISC) were identical to the CoNLL2002 and CoNLL2003 datasets, performance was - on average - lower for the WikiNER dataset. However, the general trend that stacking multiple embedding types together typically improves performance was also observed here (Fig 4.3). This was less pronounced for monolingual embeddings based on CamemBERT or multilingual embeddings based on mBERT, where performance was already relatively high at baseline (i.e. when no additional static or task-specific embeddings were included) (Table 4.2). Figure 4.3: Test-set micro-average precision, recall and F1-score for monolingual (top) and multilingual (bottom) embeddings on the French WikiNER task. The results are grouped according to the constituents of each (stacked) embedding, consistent with Table ??. The contextualized embedding type, if present, is indicated on the different panels from left to right. The stacked/task-specific embedding type (if present) is shown on the x-axis. For clarity, the results for the language-independent OHE and Char embeddings are indicated on both the upper (monolingual) and lower (multilingual) panels. Char: character embeddings; OHE: One Hot Embeddings; BPEmb: BytePair embeddings; fastT: fastText embeddings; All: all embeddings indicated on the x-axis for this panel concatenated into a single vector. Table 4.3: Overview of results acquired on the French WikiNER task with monolingual and multilingual NER systems. Note that we used a subset consisting of only 10 % of the entire WikiNER dataset. A complete overview of all results is provided in Table ??. Description Precision Recall F1-score Monolingual SpaCy fr 82.2 81.6 81.9 Flair 87.9 87.7 87.8 84.0 83.9 83.9 CamemBERT 84.5 84.8 84.6 CamemBERT + all-fr 86.3 86.4 86.3 Flair 79.1 80.0 79.4 Flair + all-fr 85.2 85.3 85.2 CamemBERT + Flair 86.2 86.2 86.1 CamemBERT + Flair + all-fr 87.4 87.4 Multilingual SpaCy XX 80.3 79.5 79.9 80.2 80.8 80.4 mBERT 85.5 85.4 85.4 mBERT + all 85.3 85.0 85.1 mFlair 74.4 75.7 74.7 mFlair + all 81.0 81.6 81.2 mBERT + mFlair 84.8 84.9 84.8 mBERT + mFlair + all 86.2 86.3 References a GitHub: flairNLP/flair/issues/238 b GitHub: flairNLP/flair/issues/238 c spacy.io/models/xx In general - for all three monolingual datasets - high-dimensional vectors based on contextualized representations obtained from pretrained LMs clearly improved performance as compared to high-dimensional stackings of static and task-specific representations. BERT-based embeddings generally outperformed Flair-based embeddings, except for monolingual English NER, where Flair- and BERT-based embeddings performed similarly. 4.1.4 Trilingual NER As expected, monolingual embeddings outperformed multilingual embeddings on the aforementioned (monolingual) NER tasks, which might reflect the lack of language-specific knowledge of the multilingual embeddings. The difference, however, was small for multilingual representations based on mBERT embeddings. To test the hypothesis that the multilingual knowledge acquired by mBERT during pretraining is particularly beneficial when the input data is multilingual, we combined random subsets of the CoNLL2002, CoNLL2003 and WikiNER dataset into a single, multilingual dataset. We evaluated NER performance using all monolingual and multilingual embeddings used for the monolingual NER tasks. Evaluating the different monolingual embeddings on the multilingual dataset provides an indication on how NER performance might suffer from incorrect predictions by an upstream language-prediction system in a document annotation pipeline like Metamaze (cf supra). For clarity, only the F1-scores of a subset of the results are included in Fig 4.4. Consistently with previous results, the precision, recall and F1-score for either the different contextualized embeddings as such, or concatenated with all monolingual or multilingual static/task-specific embeddings are provided in Table 4.4. Task-specific representations (OHE and Character embeddings) performed, as expected, poorly, with F1-scores of 60.4 % and 60.8 %, respectively (Table ??). Adding contextualized embeddings from pretrained LMs improved results drastically, again demonstrating that the acquired knowledge of the LM provided representations that encoded information highly relevant for the NER classifier to perform well - even when this LM was trained in the language that represented merely one third of the entire dataset. For all monolingual embeddings, concatenating both Flair and BERT embeddings together improved performance as compared to the representations obtained from either Flair- or BERT-based embeddings alone (Table 4.4). When increasing the dimensionality of the BERT + Flair stacked embeddings further through concatenating fastText and BytePair embeddings (in the respective languages) and task-specific OHE and character embeddings, performance increased further to roughly 85 % for all monolingual systems (Fig 4.4). Interestingly, even mBERT embeddings alone performed better on this multilingual dataset (F1-score: 86.5 %), confirming our hypothesis that, indeed, pretraining on a multilingual dataset improves performance for multilingual NER as compared to the monolingual LMs. An additional (slight) performance increase was achieved through stacking mBERT embeddings with mBytePair, OHE and character embeddings (F1-score: 87.5 %) (Table 4.4). The performance of mFlair-based embeddings was again low, in general. Not only compared to mBERT-based embeddings, but even compared to monolingual embeddings containing no BERT- or Flair-based embeddings at all (Figure 4.4). Figure 4.4: Test-set micro-average precision, recall and F1-score for monolingual (top) and multilingual (bottom) embeddings on the multilingual (English + Dutch + French) benchmark dataset. The results are grouped according to the constituents of each (stacked) embedding, consistent with Table ??. The contextualized embedding type, if present, is indicated on the different panels from left to right. The stacked/task-specific embedding type (if present) is shown on the x-axis. For clarity, only the results for experiments that did not include any static/task-specific embeddings (None) and for the experiments where all static/task-specific embeddings were included (All) are shown. No CE: No contextualized embeddings Table 4.4: Results obtained on the multilingual dataset, consisting of a random sample of the aformentioned English, Dutch and French benchmark datasets. A complete overview of all results is provided in Table ??. Embedding type Precision Recall F1-score Monolingual English embeddings 84.4 83.8 84.0 BERT 82.0 81.0 81.4 BERT + all-en 85.0 84.2 84.6 Flair 79.5 79.2 79.2 Flair + all-en 85.2 85.1 85.1 BERT + Flair 83.5 83.4 83.4 BERT + Flair + all-en 85.3 85.1 Monolingual Dutch embeddings 83.5 82.9 83.1 BERTje 80.1 77.4 78.6 BERTje + all-nl 85.0 84.8 84.9 Flair 79.1 79.7 79.3 Flair + all-nl 84.1 84.2 84.1 BERTje + Flair 80.7 80.6 80.6 BERTje + Flair + all-nl 85.2 85.8 Monolingual French embeddings 83.8 83.0 83.3 CamemBERT 81.0 79.6 80.2 CamemBERT + all-fr 86.1 85.6 Flair 76.5 75.8 76.1 Flair + all-fr 84.4 83.3 83.8 CamemBERT + Flair 82.0 81.2 81.6 CamemBERT + Flair + all-fr 86.0 85.5 85.7 Multilingual embeddings 80.2 79.1 79.6 mBERT 86.6 86.4 86.5 mBERT + all 87.6 87.4 mFlair 76.8 77.0 76.7 mFlair + all 81.2 81.5 81.3 mBERT + mFlair 86.6 86.9 86.7 mBERT + mFlair + all 86.9 86.9 86.9 4.2 Faktion datasets As discussed in chapter 3, the Faktion datasets were considerably smaller and more noisy compared to the benchmark datasets presented earlier. Therefore, it was not very surprising that performance on the Dutch Faktion dataset was - on average - much lower as compared to the CoNLL 2002 task (Fig 4.5, Table 4.5). However, there was a clear advantage of using pretrained language models to provide word embeddings. Indeed, fixed (sub)word embeddings such as monolingual BytePair or fastText embeddings performed poorly with F1-scores of roughly 50 % (Fig 4.5). On the other hand, performance did benefit from concatenating these fixed (sub)word embeddings with contextualized word vectors obtained from - especially - the pretrained Dutch Flair model. This was in contrast to the results on the CoNLL2002 task, where Flair-based embeddings were clearly outperformed by embeddings obtained from BERTje (Fig 4.2). Even though some of the embeddings seemed to benefit from further increasing the dimensionality of the vectors with trainable character-feature representations, there was no clear pattern accross the types of contextualized embeddings or across monolingual and multilingual embeddings. This, combined with the observation that character-feature embeddings alone performed very poorly, might indicate that this specific representation was not very informative for the BiLSTM-CRF classifier. The fact that the dataset was not completely monolingual is further reflected in the results for the multilingual embeddings: not only was the performance of the mBERT/mFlair-based embeddings on a par with the monolingual embeddings (Table 4.5), multilingual BytePair embeddings also clearly outperformed their monolingual counterparts (Fig ??). Performance was, on average, highest for the monolingual Flair embeddings, concatenated with all monolingual static and task-specific representations (F1-score: 72.0 ± 4.8 %). Among all multilingual embeddings, the best performance was, on average, obtained with the concatenation of all multilingual static and task-specific representations (F1-score: 70.8 ± 4.8 %) (Table 4.5). A complete overview of all results is given in Table ??. Figure 4.5: Test-set micro-average precision, recall and F1-score for monolingual (top) and multilingual (bottom) embeddings on the Dutch Faktion NER task. The results are grouped according to the constituents of each (stacked) embedding (Table ??). The contextualized embedding type, if present, is indicated on the different panels from left to right. The stacked/task-specific embedding type (if present) is shown on the x-axis. For clarity, the results for the language-independent OHE and Char embeddings are indicated on both the upper (monolingual) and lower (multilingual) panels. The symbol and error bars indicate the mean and standard error of the mean, respectively. Char: character embeddings; OHE: One Hot Embeddings; BPEmb: BytePair embeddings; fastT: fastText embeddings; All: all embeddings indicated on the x-axis for this panel concatenated into a single vector. Table 4.5: Results acquired on the Dutch Faktion NER task with both monolingual and multilingual NER systems. Results are reported as mean ± standard error of the mean (k = 5). A complete overview of all results is provided in Table ??. Embedding type Precision Recall F1-score Monolingual embeddings 76.4 ± 7.5 50.2 ± 6.1 59.8 ± 5.4 BERTje 81.5 ± 3.8 51.5 ± 4.8 62.3 ± 3.7 BERTje + all 79.1 ± 2.9 56.2 ± 4.6 65.2 ± 3.1 Flair 71.6 ± 5.2 59.9 ± 7.1 64.8 ± 6.1 Flair + all 78.7 ± 7.3 67.8 ± 6.1 BERTje + Flair 79.0 ± 4.0 50.9 ± 4.1 61.3 ± 2.9 BERTje + Flair + all 84.6 ± 2.2 60.2 ± 5.4 69.6 ± 3.3 Multilingual embeddings 84.7 ± 5.0 62.2 ± 7.3 mBERT 80.5 ± 5.0 57.3 ± 5.4 66.5 ± 5.1 mBERT + all 81.6 ± 4.6 58.5 ± 5.7 67.7 ± 4.7 mFlair 81.4 ± 5.3 51.0 ± 4.1 62.4 ± 4.3 mFlair + all 82.2 ± 5.0 60.4 ± 7.8 68.3 ± 5.7 mBERT + mFlair 87.0 ± 3.3 53.8 ± 9.2 65.1 ± 8.0 mBERT + mFlair + all 80.4 ± 5.6 60.9 ± 6.5 69.0 ± 5.9 The performance on the French Faktion dataset was, on average, lower in comparison with the Dutch dataset. However, it should be noted that the French dataset consisted of 2 entity categories while the Dutch dataset only contained a single entiy class to predict. The high-dimensional concatenations of CamemBERT together with all static and task-specific embeddings performed best, on average (F1-score: 54.6 ± 4.8 %). In line with the previous observations, the differences between monolingual and multilingual embeddings were negligible (Table 4.6), with the multilingual BytePair embeddings performing surprisingly well (Fig 4.7). Among all multilingual embeddings that were tested for this dataset, estimated performance was, on average, highest for the concatenations of mFlair embeddings with mBytePair, OHE and character embeddings (F1-score: 54.5 ± 5.8 %). For the benchmark datasets, the performance as expressed in terms of the F1-score typically reflected a good balance between precision and recall. This was, however, not always the case for this dataset. Indeed, while the the French Flair-embeddings performed poorly in terms of the average F1-score (42.3 ± 3.8 %), they did provide the highest precision, on average, among all monolingual systems (62.3 ± 6.6 %). This is a direct result of the fact that the F1-score is defined as the harmonic mean of precision and recall, with the latter being low, on average, for this system (32.2 ± 2.8 %), resulting in a low overall F1-score. For specific applications that require either a high precision or recall, it might make sense in these situations to not only consider their harmonic mean alone, but also evaluate the performance of the system in terms of precision and recall separately. An overview of all results is given in Table ??. Figure 4.6: Test-set micro-average precision, recall and F1-score for monolingual (top) and multilingual (bottom) embeddings on the French Faktion NER task. The results are grouped according to the constituents of each (stacked) embedding (Table ??). The contextualized embedding type, if present, is indicated on the different panels from left to right. The stacked/task-specific embedding type (if present) is shown on the x-axis. For clarity, the results for the language-independent OHE and Char embeddings are indicated on both the upper (monolingual) and lower (multilingual) panels. The symbol and error bars indicate the mean and standard error of the mean, respectively. Char: character embeddings; OHE: One Hot Embeddings; BPEmb: BytePair embeddings; fastT: fastText embeddings; All: all embeddings indicated on the x-axis for this panel concatenated into a single vector. Table 4.6: Results acquired on the French Faktion NER task with both monolingual and multilingual NER systems. Results are reported as mean ± standard error of the mean (k = 5). A complete overview of all results is provided in Table ??. Embedding type Precision Recall F1-score Monolingual embeddings 53.7 ± 3.6 44.9 ± 4.0 48.6 ± 3.5 CamemBERT 49.9 ± 5.4 26.9 ± 3.2 34.7 ± 3.8 CamemBERT + all 60.1 ± 6.3 50.5 ± 5.0 Flair 62.3 ± 6.6 32.2 ± 2.8 42.3 ± 3.8 Flair + all 56.2 ± 5.2 47.7 ± 5.1 51.4 ± 5.1 CamemBERT + Flair 60.2 ± 4.5 35.8 ± 4.9 44.6 ± 5.1 CamemBERT + Flair + all 59.6 ± 5.4 50.9 ± 4.7 54.6 ± 4.8 Multilingual embeddings 52.1 ± 5.4 44.6 ± 3.9 47.6 ± 4.1 mBERT 62.3 ± 7.3 39.7 ± 3.4 48.3 ± 4.7 mBERT + all 59.9 ± 4.4 49.0 ± 3.8 53.7 ± 3.8 mFlair 50.3 ± 5.8 28.6 ± 5.2 36.2 ± 5.8 mFlair + all 60.6 ± 6.6 49.6 ± 5.3 mBERT + mFlair 62.4 ± 2.6 41.5 ± 2.7 49.6 ± 2.2 mBERT + mFlair + all 61.4 ± 3.7 47.0 ± 2.7 53.0 ± 2.7 Lastly, a bilingual dataset was constructed by merging the entire Dutch and French Faktion datasets. Consistently with the results on the multilingual benchmark dataset, we provide the F1-scores for a subset of embedding types in Figure 4.7 and the precision, recall and F1-scores for this subset in Table 4.7. Multilingual stacked embeddings (e.g. mBERT, mBPEmb and character-feature embeddings) outperformed (either French or Dutch) monolingual embeddings (Table 4.7), with the the concatenation of mBERT, mBytePair and character embeddings performing best, on average (F1-score: 62.8 ± 1.9 %, ??). These results are in line with the results obtained on the multilingual benchmark dataset, thus confirming our research hypothesis that these multilingual embeddings are more effective than monolingual embeddings when the input data itself is multilingual. A complete overview of all results is provided in Table ??. Table 4.7: Results acquired on the multilingual (Dutch + French) Faktion NER task with both monolingual and multilingual NER systems. Results are reported as mean ± standard error of the mean (k = 5). A complete overview of all results is provided in Table ??. Embedding type Precision Recall F1-score Monolingual Dutch embeddings 67.3 ± 3.4 51.7 ± 4.3 58.3 ± 3.9 BERTje 67.8 ± 3.6 32.5 ± 1.8 43.7 ± 1.6 BERTje + all-nl 65.9 ± 2.8 41.0 ± 3.3 50.2 ± 2.8 Flair 67.1 ± 2.6 43.9 ± 3.6 52.7 ± 2.8 Flair + all-nl 69.8 ± 2.1 54.5 ± 1.6 BERTje + Flair 67.1 ± 2.1 28.7 ± 1.8 40.1 ± 2.0 BERTje + Flair + all-nl 67.7 ± 2.6 39.5 ± 3.3 49.7 ± 3.0 Monolingual French embeddings 66.0 ± 2.6 51.5 ± 3.4 57.7 ± 3.0 CamemBERT 52.8 ± 5.2 22.2 ± 1.6 30.9 ± 1.8 CamemBERT + all-fr 67.4 ± 3.6 53.4 ± 3.0 Flair 61.9 ± 4.1 28.0 ± 2.9 38.0 ± 2.8 Flair + all-fr 68.4 ± 3.9 49.9 ± 3.9 57.3 ± 3.3 CamemBERT + Flair 72.0 ± 4.4 40.6 ± 3.8 51.6 ± 3.7 CamemBERT + Flair + all-fr 68.3 ± 1.8 51.8 ± 3.0 58.6 ± 1.6 Multilingual embeddings 73.7 ± 4.6 53.3 ± 3.4 mBERT 68.1 ± 2.3 45.8 ± 2.6 54.6 ± 2.3 mBERT + all 72.3 ± 3.2 52.3 ± 2.9 60.2 ± 2.0 mFlair 68.2 ± 4.6 35.5 ± 3.1 46.3 ± 2.9 mFlair + all 69.0 ± 2.9 50.8 ± 3.9 58.3 ± 3.3 mBERT + mFlair 71.0 ± 2.6 41.8 ± 1.6 52.5 ± 1.5 mBERT + mFlair + all 71.1 ± 3.3 51.2 ± 3.2 59.4 ± 3.0 Figure 4.7: Test-set micro-average precision, recall and F1-score for monolingual (top) and multilingual (bottom) embeddings on the multilingual (Dutch + French) Faktion dataset. The results are grouped according to the constituents of each (stacked) embedding (Table ??). The contextualized embedding type, if present, is indicated on the different panels from left to right. The stacked/task-specific embedding type (if present) is shown on the x-axis. For clarity, only the results for experiments that did not include any static/task-specific embeddings (None) and for the experiments where all static/task-specific embeddings were included (All), are shown. No CE: No contextualized embeddings References "]
]
